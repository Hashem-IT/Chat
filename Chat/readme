daten bank mit Docker :
     iamge bilden
      //befehl  bau      name    hier
        1- docker build -t mychat .
      // image zu zeigen
        docker images
      //docker bauen
      // befehl run      port docker : host chat:continer(name von mir) chat-app: image(name was in docker build erstellt)
     2-   docker run -itd -p 8000:8000 --name chat mychat

     docker ps
     docker stop ID => stopt Docker und braucht name von docker
     docker start ID => start socker und braucht name von docker

daten bank mit Docker Compose:

    docker-compose.yml => erstellen mit details rein
    application.properties =>  erstellen mit details rein
    docker-compose up -d    => db starten , mit -d läft docker in hinter, ohne bleibt in Terminal run


Docker Swarm /https://dev.to/adityapratapbh1/setting-up-a-docker-swarm-cluster-and-deploying-containers-a-comprehensive-guide-1gco/
    Terminal java
        docker push mychat => möglicherweisehochladen
        docker tag mychat mychat1 => um ein neues Tag für ein vorhandenes Docker zu erstellen

        test
            in docker.com in Repositories creates ein (270509/mychat)
            in Terminal
                docker login
                docker tag mychat 270509/mychat
                docker push 270509/mychat

    play with docker  Terminal
        docker login
        docker pull 270509/mychat
        docker images => zeigt 270509/mychat
        //            außer:inner(Continer)  name= continer
        docker run -i -t -d -p 8000:8000 --name=chatservice 270509/mychat
        => kommt name von image
        docker logs chatservice  // chatservice noch zu wissen
        oder : docker logs "und erste vier Buchstapen von CONTAINER ID
        hostname // um name von host
        hostname -i // addresse
        // oben steht portnummer 8000 ,link fopieren und in Rester einfügen
        http://ip172-18-0-119-cpm9n4q91nsg00dt2qig-8000.direct.labs.play-with-docker.com/restapi/users/all => das zeigt die Methode get all student

        docker stop chatservice
        docker rm chatservice // zu löschen

        *Schritt3 in tutor erstellen 3 nodes
        node1
            // machen node1 als chef
            docker swarm init --advertise-addr=192.168.0.13 // (adresse von hostname)

        node2
            docker swarm join --token=asdfghjkl
            // das kommt nach docker swarm init in node1
            docker swarm join --token SWMTKN-1-3mj55j0o0t2vjrl407143qkbflxli8glrw6oz365gdwmkw0n7w-cwxdnxjd3xqq54s9t6xp82wj5 192.168.0.13:2377
         node3
            // das kommt nach docker swarm init in node1
            docker swarm join --token SWMTKN-1-3mj55j0o0t2vjrl407143qkbflxli8glrw6oz365gdwmkw0n7w-cwxdnxjd3xqq54s9t6xp82wj5 192.168.0.13:2377

         *Schritt4 in tutor Deploying Services in master
         master node ist 1 chef
         //docker service create --name <service-name> --replicas <number-of-replicas> <image-name>
         replicas = 3 => 1 chef und 2 worker
         $ docker service create --name=chat-sevr-swarm -p 80:8000 --replicas=3 270509/mychat
         //danach in node 2 und 3 mit "docker ps" sieht man es
         //  --replicas=2 mit 2 , nimmt node1 als chef und worker
         // jetzt in postman
            http://ip172-18-0-66-cpm9n4q91nsg00dt2qig-80.direct.labs.play-with-docker.com/restapi/users/all  => geht
         // um zu skalen
         docker service scale chat-sevr-swarm=5


Asynchrones Messaging
    docker kafka
        -in dependency erstmal schreiben
        - CMD 1
            docker network create -d bridge kafka-network
            // um network zu erstellen (brücke), wir brauchen nur netzwerkname(kafka-network)

            docker run -itd --name kafka-server --hostname kafka-server --network kafka-network -e KAFKA_CFG_NODE_ID=0 -e KAFKA_CFG_PROCESS_ROLES=controller,broker -e KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 -e KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT -e KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka-server:9093 -e KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER bitnami/kafka:latest
            //

            docker run --network=kafka-network --rm -it bitnami/kafka kafka-topics.sh --create --topic my-topic-1 --bootstrap-server kafka-server:9092
            //neu Tobic anzulegen, müssen von rien schreiben ,

            docker run --network=kafka-network --rm -it bitnami/kafka kafka-console-producer.sh --topic my-topic-1 --bootstrap-server kafka-server:9092
            // das ist producer , wo kann man schreiben und bei andere CMD nur lesen , das ist ein Image

        - CMD 2
            docker run --network=kafka-network --rm -it bitnami/kafka kafka-console-consumer.sh --topic my-topic-1 --bootstrap-server kafka-server:9092 --from-beginning
            // das ist consumer , wo kann man nur lesen , das ist ein Image
        Beide treffen über Message system , eine schreibt in Tobic und andere liest von Tobic

        Creating the Kafka Producer
             public class App {

                 private static final String TOPIC = "my-kafka-topic";
                 private static final String BOOTSTRAP_SERVERS = "kafka-server:9092,localhost:9094,localhost:9095";

                 private static void produce() {
                     // Create configuration options for our producer and initialize a new producer
                     Properties props = new Properties();
                     props.put("bootstrap.servers", BOOTSTRAP_SERVERS);
                     // We configure the serializer to describe the format in which we want to produce data into
                     // our Kafka cluster
                     props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
                     props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

                     // Since we need to close our producer, we can use the try-with-resources statement to
                     // create
                     // a new producer
                     try (Producer<String, String> producer = new KafkaProducer<>(props)) {
                         // here, we run an infinite loop to sent a message to the cluster every second
                         for (int i = 0;; i++) {
                             String key = Integer.toString(i);
                             String message = "this is message " + Integer.toString(i);

                             producer.send(new ProducerRecord<String, String>(my-topic-1, key, message));

                             // log a confirmation once the message is written
                             System.out.println("sent msg " + key);
                             try {
                                 // Sleep for a second
                                 Thread.sleep(1000);
                             } catch (Exception e) {
                                 break;
                             }
                         }
                     } catch (Exception e) {
                         System.out.println("Could not start producer: " + e);
                     }
                 }
             }


        Creating the Kafka Consumer
                public class App {

                    private static final String TOPIC = "my-kafka-topic";
                    private static final String BOOTSTRAP_SERVERS = "kafka-server:9092,localhost:9094,localhost:9095";

                    private static void consume() {
                        // Create configuration options for our consumer
                        Properties props = new Properties();
                        props.setProperty("bootstrap.servers", BOOTSTRAP_SERVERS);
                        // The group ID is a unique identified for each consumer group
                        props.setProperty("group.id", "my-group-id");
                        // Since our producer uses a string serializer, we need to use the corresponding
                        // deserializer
                        props.setProperty("key.deserializer",
                                "org.apache.kafka.common.serialization.StringDeserializer");
                        props.setProperty("value.deserializer",
                                "org.apache.kafka.common.serialization.StringDeserializer");
                        // Every time we consume a message from kafka, we need to "commit" - that is, acknowledge
                        // receipts of the messages. We can set up an auto-commit at regular intervals, so that
                        // this is taken care of in the background
                        props.setProperty("enable.auto.commit", "true");
                        props.setProperty("auto.commit.interval.ms", "1000");

                        // Since we need to close our consumer, we can use the try-with-resources statement to
                        // create it
                        try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props)) {
                            // Subscribe this consumer to the same topic that we wr1ote messages to earlier
                            consumer.subscribe(Arrays.asList(TOPIC));
                            // run an infinite loop where we consume and print new messages to the topic
                            while (true) {
                                // The consumer.poll method checks and waits for any new messages to arrive for the
                                // subscribed topic
                                // in case there are no messages for the duration specified in the argument (1000 ms
                                // in this case), it returns an empty list
                                ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(1000));
                                for (ConsumerRecord<String, String> record : records) {
                                    System.out.printf("received message: %s\n", record.value());
                                }
                            }
                        }
                    }

                }



        mein kafka-chat server
        C:\Users\Yassin\Desktop\OTH\VS\chat\Chat\Chat>docker build --tag chat-kafka .
        C:\Users\Yassin\Desktop\OTH\VS\chat\Chat\Chat>docker run -itd --name chat-kafka -p 8000:8000 --network kafka-network chat-kafka


        kafka broker
        docker network create -d bridge kafka-network
        docker run -itd --name kafka-server --hostname kafka-server --network kafka-network -e KAFKA_CFG_NODE_ID=0 -e KAFKA_CFG_PROCESS_ROLES=controller,broker -e KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093 -e KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT -e KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka-server:9093 -e KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER   bitnami/kafka:latest
        docker run --network=kafka-network --rm -it bitnami/kafka kafka-topics.sh --create --topic my-topic-1 --bootstrap-server kafka-server:9092


        zum Testen:
            POST http://localhost:8000/restapi/producer/sendsingle
            in Body => Hallo test von kafak1 23 123

Code:
        soll members in der Group-Klasse eine Sammlung von userId-Strings sein, nicht User-Objekten.


        Test


